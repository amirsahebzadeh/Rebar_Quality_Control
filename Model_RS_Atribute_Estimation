#Loading Google Drive
from google.colab import drive
drive.mount('/content/drive')

#Installing Detectron2
!python -m pip install pyyaml==5.1
import sys, os, distutils.core
# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).
# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions
!git clone 'https://github.com/facebookresearch/detectron2'
dist = distutils.core.run_setup("./detectron2/setup.py")
!python -m pip install {' '.join([f"'{x}'" for x in dist.install_requires])}
sys.path.insert(0, os.path.abspath('./detectron2'))

#Import Essential Libraries
import torch
import cv2
from detectron2.config import get_cfg
from detectron2.engine import DefaultPredictor
from google.colab import drive
import os
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog

#Configuration and Instance Segmentation
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
cfg = get_cfg()
cfg.merge_from_file("/content/drive/MyDrive/Thesis- Model1/Detectron2_Models_RS/config.yaml")
cfg.MODEL.WEIGHTS = "/content/drive/MyDrive/Thesis- Model1/Detectron2_Models_RS/model_final.pth"
cfg.MODEL.DEVICE = device.type
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2  # Replace with your number of classes
predictor = DefaultPredictor(cfg)
image_path = '/content/drive/MyDrive/Thesis- Model1/Test/IMG_4654.JPG'
image = cv2.imread(image_path)
outputs = predictor(image)
v = Visualizer(image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
output_image = out.get_image()[:, :, ::-1]
cv2_imshow(output_image)

#Extracting Instances
instances = outputs["instances"].to(device)
pred_masks = instances.pred_masks  # [N, H, W]
pred_boxes = instances.pred_boxes.tensor
pred_classes = instances.pred_classes
scores = instances.scores

#Installing Depth Pro Model
!git clone https://github.com/apple/ml-depth-pro.git
%cd ml-depth-pro
!pip install -r requirements.txt
!pip install -e .
!bash get_pretrained_models.sh
import sys
sys.path.append('/content/ml-depth-pro/src')

import torch
import depth_pro
from PIL import Image
import numpy as np

#Depth Model Configuration and Transformation
model, transform = depth_pro.create_model_and_transforms()
model.eval()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
model.half()
if not isinstance(image, Image.Image):
    image = Image.open(image_path).convert("RGB")
image = image.resize((1024, 768))
image_transformed = transform(image).to(device).half()
native_focal_length_mm = #(has be adjusted based on the camera)
sensor_width_mm = #(has be adjusted based on the camera)
image_width_px = image.width
f_px = (native_focal_length_mm / sensor_width_mm) * image_width_px
f_px_tensor = torch.tensor([f_px], dtype=torch.float32, device=device)
print(f"Native focal length (mm): {native_focal_length_mm}")
print(f"Sensor width (mm): {sensor_width_mm}")
print(f"Resized image width (pixels): {image_width_px}")
print(f"Focal length in pixels: {f_px:.2f}")
with torch.no_grad():
    prediction = model.infer(image_transformed.unsqueeze(0), f_px=f_px_tensor)
    depth_map = prediction["depth"]  # Depth in meters
depth_map_np = depth_map.squeeze(0).cpu().numpy()
depth_map_np_resized = np.array(Image.fromarray(depth_map_np).resize(
    (image_width_px, int(image_width_px * image.height / image.width))
))
print("Depth map calculation completed.")

#Depth of Each Instance
pred_masks_np = pred_masks.cpu().numpy()  # Assuming pred_masks is on GPU, convert it to numpy
num_instances = pred_masks_np.shape[0]
resized_masks = []
for i in range(num_instances):
    # Convert the mask to uint8 for compatibility with OpenCV
    mask_uint8 = (pred_masks_np[i] * 255).astype(np.uint8)
    resized_mask = cv2.resize(mask_uint8, (1024, 768), interpolation=cv2.INTER_NEAREST)
    resized_masks.append(resized_mask > 0)
for i, mask in enumerate(resized_masks):
    # Apply the mask to the depth map to get the depth of this particular instance
    instance_depth = np.where(mask, depth_map_np, 0)
    mean_depth = instance_depth[mask].mean() if np.any(mask) else 0
    print(f'Instance {i+1} - Mean Depth: {mean_depth:.2f} meters')
    plt.imshow(instance_depth, cmap='plasma')
    plt.title(f'Instance {i+1} Depth (Mean Depth: {mean_depth:.2f} m)')
    plt.colorbar()
    plt.axis('off')
    plt.show()


#Rebar Diameter Estimation
image_for_vis = np.array(image) 
pred_masks_np = pred_masks.cpu().numpy()
num_instances = pred_masks_np.shape[0]
resized_masks = []
for i in range(num_instances):
    mask_uint8 = (pred_masks_np[i] * 255).astype(np.uint8)
    resized_mask = cv2.resize(mask_uint8, (1024, 768), interpolation=cv2.INTER_NEAREST)
    resized_masks.append(resized_mask > 0)  # Convert back to boolean
image_for_vis = cv2.cvtColor(image_for_vis, cv2.COLOR_RGB2BGR)
# Give a Real-world Distance to Compelete Camera Calibration Process
real_world_diameter = #(adjusted based on the data)
#Calculate Calibration Factor Using the First Instance
mask_1 = resized_masks[0].astype(np.uint8)
contours, _ = cv2.findContours(mask_1, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
if len(contours) > 0:
    cnt = max(contours, key=cv2.contourArea)
    x, y, w, h = cv2.boundingRect(cnt)
    y_middle = y + h // 2
    points = [x_coord for x_coord in range(x, x + w) if mask_1[y_middle, x_coord]]
    if len(points) >= 2:
        diameter_px = points[-1] - points[0]
        calibration_factor = real_world_diameter / diameter_px
        print(f"Calibration Factor: {calibration_factor:.6f} meters/pixel")
# Step 2: Process Instances in the Specified Order (Can be changed)
instance_order = [3, 4, 1, 0, 2]  # 4 -> 5 -> 2 -> 1 -> 3 (0-based indexing)
for idx, i in enumerate(instance_order):
    mask = resized_masks[i]
    mask_uint8 = mask.astype(np.uint8)
    # Find contours
    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if len(contours) > 0:
        # Largest contour assumed as the primary object
        cnt = max(contours, key=cv2.contourArea)
        x, y, w, h = cv2.boundingRect(cnt)
        # Calculate the middle y-coordinate for the horizontal line
        y_middle = y + h // 2
        # Find Intersection Points Along the Horizontal Line
        points = [x_coord for x_coord in range(x, x + w) if mask[y_middle, x_coord]]
        if len(points) >= 2:
            point1, point2 = points[0], points[-1]
            diameter_px = point2 - point1
            # Apply Calibration Factor to Calculate Real-world Diameter
            diameter_m = diameter_px * calibration_factor
            print(f'Instance {idx+1},Diameter: {diameter_m:.4f} meters')
            cv2.line(image_for_vis, (point1, y_middle), (point2, y_middle), (0, 255, 0), 2)
            cv2.circle(image_for_vis, (point1, y_middle), 5, (0, 0, 255), -1)
            cv2.circle(image_for_vis, (point2, y_middle), 5, (0, 0, 255), -1)
            text = f'Instance {idx+1}, Diameter: {diameter_m:.4f}m'
            centroid = (int((point1 + point2) / 2), y_middle - 10)
            cv2.putText(image_for_vis, text, centroid, cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 0, 0), 1)
image_for_vis = cv2.cvtColor(image_for_vis, cv2.COLOR_BGR2RGB)
plt.figure(figsize=(15, 10))
plt.imshow(image_for_vis)
plt.title('Instance Segmentation with Corrected Diameter Measurement')
plt.axis('off')
plt.show()


#Stirrup Diameter Estimation
image_for_vis = np.array(image)  # Convert the PIL image to a numpy array (RGB format)
pred_masks_np = pred_masks.cpu().numpy()  # Assuming `pred_masks` is on GPU
num_instances = pred_masks_np.shape[0]
resized_masks = []
for i in range(num_instances):
    mask_uint8 = (pred_masks_np[i] * 255).astype(np.uint8)
    resized_mask = cv2.resize(mask_uint8, (1024, 768), interpolation=cv2.INTER_NEAREST)
    resized_masks.append(resized_mask > 0)  # Convert back to boolean
image_for_vis = cv2.cvtColor(image_for_vis, cv2.COLOR_RGB2BGR)
#Give a Real-world Distance to Compelete Camera Calibration Process
real_world_diameter = (has to be adjusted)
# Step 1: Calculate the Calibration Factor Using the First Horizontal Instance
horizontal_instance_idx = 8
mask_calib = resized_masks[horizontal_instance_idx]
contours, _ = cv2.findContours(mask_calib.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
if len(contours) > 0:
    cnt = max(contours, key=cv2.contourArea)
    x, y, w, h = cv2.boundingRect(cnt)
    x_middle = x + w // 2
    points = [y_coord for y_coord in range(y, y + h) if mask_calib[y_coord, x_middle]]
    if len(points) >= 2:
        diameter_px = points[-1] - points[0]
        calibration_factor = real_world_diameter / diameter_px
        print(f"Calibration Factor: {calibration_factor:.6f} meters/pixel")
# Step 2: Process Horizontal Instances
horizontal_instances = [11, 8, 7, 9, 5, 10]  # Indexes for instances 12, 9, 8, 10, 6, 11
for idx, i in enumerate(horizontal_instances):
    mask = resized_masks[i]
    mask_uint8 = mask.astype(np.uint8)
    # Find contours
    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if len(contours) > 0:
        cnt = max(contours, key=cv2.contourArea)
        x, y, w, h = cv2.boundingRect(cnt)
        # Calculate the Middle X-coordinate For the Vertical Line
        x_middle = x + w // 2
        # Find Intersection Points Along the Vertical Line
        points = [y_coord for y_coord in range(y, y + h) if mask[y_coord, x_middle]]
        if len(points) >= 2:
            point1, point2 = points[0], points[-1]
            diameter_px = point2 - point1
            # Apply calibration factor to calculate real-world diameter
            diameter_m = diameter_px * calibration_factor
            print(f'Instance {idx+1} (Original {i+1}) - Diameter: {diameter_m:.4f} meters')
            cv2.line(image_for_vis, (x_middle, point1), (x_middle, point2), (0, 255, 0), 2)
            cv2.circle(image_for_vis, (x_middle, point1), 5, (0, 0, 255), -1)
            cv2.circle(image_for_vis, (x_middle, point2), 5, (0, 0, 255), -1)
            text = f'Instance {idx+1} (Orig {i+1}), Diameter: {diameter_m:.4f}m'
            centroid = (x_middle + 10, int((point1 + point2) / 2))
            cv2.putText(image_for_vis, text, centroid, cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 0, 0), 1)
image_for_vis = cv2.cvtColor(image_for_vis, cv2.COLOR_BGR2RGB)
plt.figure(figsize=(15, 10))
plt.imshow(image_for_vis)
plt.title('Diameter Measurement for Horizontal Instances')
plt.axis('off')
plt.show()



#Length of Instances
image_for_vis = np.array(image)  # Convert PIL image to numpy (RGB format)
pred_masks_np = pred_masks.cpu().numpy()
num_instances = pred_masks_np.shape[0]  # Dynamically get the number of instances
resized_masks = []
for i in range(num_instances):
    mask_uint8 = (pred_masks_np[i] * 255).astype(np.uint8)
    resized_mask = cv2.resize(mask_uint8, (1024, 768), interpolation=cv2.INTER_NEAREST)
    resized_masks.append(resized_mask > 0)
image_for_vis = cv2.cvtColor(image_for_vis, cv2.COLOR_RGB2BGR)
for i, mask in enumerate(resized_masks):
    mask_uint8 = mask.astype(np.uint8)
    #Finding Contours and Bounding Box
    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if len(contours) > 0:
        cnt = max(contours, key=cv2.contourArea)
        x, y, w, h = cv2.boundingRect(cnt)  # Bounding box: x, y, width, height
        #Determine if the Instance is Vertical or Horizontal
        is_vertical = h > w  # True if height > width
        dimension_px = h if is_vertical else w  # Length in pixels: height for vertical, width for horizontal
        #Calculate Mean Depth of the Mask
        instance_depth = np.where(mask, depth_map_np, 0)
        mean_depth = instance_depth[mask].mean() if np.any(mask) else 0
        #Calculate Real-world Length
        length_m = (dimension_px * mean_depth) / f_px if f_px > 0 and mean_depth > 0 else 0
        orientation = "Vertical" if is_vertical else "Horizontal"
        print(f'Instance {i+1} ({orientation}) - Length: {length_m:.2f} meters')
        cv2.rectangle(image_for_vis, (x, y), (x + w, y + h), (0, 255, 0), 2)
        text = f'Instance {i+1}: {orientation} Length {length_m:.2f}m'
        text_position = (x, y - 10 if y - 10 > 10 else y + 10)
        cv2.putText(image_for_vis, text, text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 0, 0), 1)
image_for_vis = cv2.cvtColor(image_for_vis, cv2.COLOR_BGR2RGB)
plt.figure(figsize=(15, 10))
plt.imshow(image_for_vis)
plt.title('Lengths of Instances')
plt.axis('off')
plt.show()



#Alignment of Instances
image_for_vis = np.array(image)  # Convert the PIL image to a numpy array (RGB format)
print("Converting masks to numpy format.")
pred_masks_np = pred_masks.cpu().numpy()  # Assuming `pred_masks` is on GPU
num_instances = pred_masks_np.shape[0]
resized_masks = []
print("Resizing each mask to match the depth map size.")
for i in range(num_instances):
    mask_uint8 = (pred_masks_np[i] * 255).astype(np.uint8)
    resized_mask = cv2.resize(mask_uint8, (1024, 768), interpolation=cv2.INTER_NEAREST)
    resized_masks.append(resized_mask > 0)  # Convert back to boolean
print("Converting the original image to BGR for further processing.")
image_for_vis = cv2.cvtColor(image_for_vis, cv2.COLOR_RGB2BGR)
print("Calculating alignment angles and matching line length to each instance size.")
for i, mask in enumerate(resized_masks, start=1):  # Process all instances
    mask_uint8 = mask.astype(np.uint8)
    # Finding Contours
    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if len(contours) == 0:
        print(f"No contours. Skipping.")
        continue
    cnt = max(contours, key=cv2.contourArea)
    if len(cnt) < 5:
        print(f"Contour for instance {i} has too few points. Skipping.")
        continue
    # Fitting a Line to the Contour Points
    [vx, vy, x, y] = cv2.fitLine(cnt, cv2.DIST_L2, 0, 0.01, 0.01)
    # Get the Bounding Box Dimensions
    x_min, y_min, w, h = cv2.boundingRect(cnt)  # Bounding box: x, y, width, height
    # Determine Line Length Based on the Instance Orientation
    length = h if h > w else w
    x1 = int(x - length * vx / 2)
    y1 = int(y - length * vy / 2)
    x2 = int(x + length * vx / 2)
    y2 = int(y + length * vy / 2)
    angle_radians = math.atan2(vy, vx)
    angle_degrees = math.degrees(angle_radians)
    print(f"Instance {i} - Alignment Angle: {angle_degrees:.2f} degrees")
    cv2.line(image_for_vis, (x1, y1), (x2, y2), (0, 0, 255), 2)  # Red line for fitted line
    text = f'{angle_degrees:.2f}°'
    text_position = (x_min, y_min - 10 if y_min - 10 > 10 else y_min + 10)
    cv2.putText(image_for_vis, text, text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)
print("Converting back to RGB for final display using matplotlib.")
image_for_vis = cv2.cvtColor(image_for_vis, cv2.COLOR_BGR2RGB)
plt.figure(figsize=(15, 10))
plt.imshow(image_for_vis)
plt.title('Instance Alignment Angles')
plt.axis('off')
plt.show()


#Spacing between Instances
image_for_vis = np.array(image)  # Convert the PIL image to a numpy array (RGB format)
pred_masks_np = pred_masks.cpu().numpy()
num_instances = pred_masks_np.shape[0]
resized_masks = []
for i in range(num_instances):
    mask_uint8 = (pred_masks_np[i] * 255).astype(np.uint8)
    resized_mask = cv2.resize(mask_uint8, (1024, 768), interpolation=cv2.INTER_NEAREST)
    resized_masks.append(resized_mask > 0)
image_for_vis = cv2.cvtColor(image_for_vis, cv2.COLOR_RGB2BGR)
# Instances that the distance between them has to be calculated
mask_4 = resized_masks[3]
mask_5 = resized_masks[4]
fixed_y = image_for_vis.shape[0] // 2
# Find the rightmost point of left instance (here instance 4)
rightmost_x_instance_4 = max(x for x in range(mask_4.shape[1]) if mask_4[fixed_y, x])
# Find the leftmost point of right instance (here instance 5)
leftmost_x_instance_5 = min(x for x in range(mask_5.shape[1]) if mask_5[fixed_y, x])
spacing_px = leftmost_x_instance_5 - rightmost_x_instance_4
#Calibration factor (This can be calculated from diameter estimation part)
spacing_m = spacing_px * calibration_factor
print(f"Pixel Distance: {spacing_px} pixels")
print(f"Real-World Spacing: {spacing_m:.4f} meters")
print(f"Calibration Factor: {calibration_factor:.6f} meters/pixel")
cv2.circle(image_for_vis, (rightmost_x_instance_4, fixed_y), 5, (0, 0, 255), -1)  # Red for instance 4
cv2.circle(image_for_vis, (leftmost_x_instance_5, fixed_y), 5, (255, 0, 0), -1)  # Blue for instance 5
cv2.line(image_for_vis, (rightmost_x_instance_4, fixed_y), (leftmost_x_instance_5, fixed_y), (0, 255, 0), 2)  # Green line
midpoint = ((rightmost_x_instance_4 + leftmost_x_instance_5) // 2, fixed_y - 10)
text = f"Spacing: {spacing_m:.3f} m"
cv2.putText(image_for_vis, text, midpoint, cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
image_for_vis = cv2.cvtColor(image_for_vis, cv2.COLOR_BGR2RGB)
plt.figure(figsize=(15, 10))
plt.imshow(image_for_vis)
plt.title("Spacing Between Instance 4 and Instance 5")
plt.axis('off')
plt.show()
